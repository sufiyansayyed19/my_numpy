{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Saving and Loading Arrays\n",
                "\n",
                "**Module 06 | Notebook 01**\n",
                "\n",
                "---\n",
                "\n",
                "## Objective\n",
                "By the end of this notebook, you will master:\n",
                "- Saving arrays to .npy and .npz files\n",
                "- Loading arrays from files\n",
                "- Multiple arrays in single file\n",
                "- Memory-mapped files for large data\n",
                "- Best practices for data persistence"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import os\n",
                "np.set_printoptions(precision=2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. np.save() and np.load() - Single Array"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save a single array to .npy file\n",
                "arr = np.arange(10)\n",
                "print(f\"Array to save: {arr}\")\n",
                "\n",
                "np.save('my_array.npy', arr)\n",
                "print(\"Saved to my_array.npy\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the array\n",
                "loaded = np.load('my_array.npy')\n",
                "print(f\"Loaded array: {loaded}\")\n",
                "print(f\"Same as original: {np.array_equal(arr, loaded)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# .npy extension added automatically if missing\n",
                "np.save('test_array', arr)  # Creates test_array.npy\n",
                "print(f\"File exists: {os.path.exists('test_array.npy')}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# What's in a .npy file?\n",
                "# It's a binary format with:\n",
                "# - Magic number\n",
                "# - Version info\n",
                "# - Header with dtype, shape, order\n",
                "# - Raw data bytes\n",
                "\n",
                "print(f\"File size: {os.path.getsize('my_array.npy')} bytes\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. np.savez() - Multiple Arrays"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save multiple arrays to a single .npz file\n",
                "arr1 = np.arange(10)\n",
                "arr2 = np.random.rand(3, 4)\n",
                "arr3 = np.array(['a', 'b', 'c'])\n",
                "\n",
                "# Named arrays\n",
                "np.savez('multiple_arrays.npz', \n",
                "         integers=arr1, \n",
                "         floats=arr2, \n",
                "         strings=arr3)\n",
                "\n",
                "print(\"Saved multiple arrays to .npz\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load .npz file\n",
                "loaded = np.load('multiple_arrays.npz')\n",
                "\n",
                "# It's like a dictionary\n",
                "print(f\"Keys: {list(loaded.keys())}\")\n",
                "print(f\"integers: {loaded['integers']}\")\n",
                "print(f\"floats shape: {loaded['floats'].shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Close the file (good practice)\n",
                "loaded.close()\n",
                "\n",
                "# Or use context manager\n",
                "with np.load('multiple_arrays.npz') as data:\n",
                "    print(f\"Keys: {list(data.keys())}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Unnamed arrays (automatic names: arr_0, arr_1, ...)\n",
                "np.savez('unnamed.npz', arr1, arr2, arr3)\n",
                "\n",
                "with np.load('unnamed.npz') as data:\n",
                "    print(f\"Auto-generated keys: {list(data.keys())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. np.savez_compressed() - Compressed Archives"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a larger array for comparison\n",
                "large_arr = np.random.rand(1000, 1000)\n",
                "\n",
                "# Uncompressed\n",
                "np.savez('uncompressed.npz', data=large_arr)\n",
                "\n",
                "# Compressed\n",
                "np.savez_compressed('compressed.npz', data=large_arr)\n",
                "\n",
                "print(f\"Uncompressed size: {os.path.getsize('uncompressed.npz') / 1e6:.2f} MB\")\n",
                "print(f\"Compressed size: {os.path.getsize('compressed.npz') / 1e6:.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compression is more effective for structured data\n",
                "structured_arr = np.zeros((1000, 1000))  # Lots of repeated values\n",
                "\n",
                "np.savez('struct_uncompressed.npz', data=structured_arr)\n",
                "np.savez_compressed('struct_compressed.npz', data=structured_arr)\n",
                "\n",
                "print(f\"Zeros uncompressed: {os.path.getsize('struct_uncompressed.npz') / 1e6:.2f} MB\")\n",
                "print(f\"Zeros compressed: {os.path.getsize('struct_compressed.npz') / 1e3:.2f} KB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Memory-Mapped Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For very large files, use memory mapping\n",
                "# File is accessed from disk on demand, not loaded entirely\n",
                "\n",
                "# Create a large array and save it\n",
                "large = np.arange(1000000).reshape(1000, 1000)\n",
                "np.save('large_array.npy', large)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load with memory mapping\n",
                "mmap = np.load('large_array.npy', mmap_mode='r')  # read-only\n",
                "\n",
                "print(f\"Shape: {mmap.shape}\")\n",
                "print(f\"First row: {mmap[0, :5]}\")\n",
                "\n",
                "# The entire array is NOT loaded into RAM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Memory map modes:\n",
                "# 'r'  - read-only\n",
                "# 'r+' - read-write (changes saved to disk)\n",
                "# 'w+' - create new file for read-write\n",
                "# 'c'  - copy-on-write (changes not saved)\n",
                "\n",
                "mmap_rw = np.load('large_array.npy', mmap_mode='r+')\n",
                "# mmap_rw[0, 0] = 999  # Would modify the file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create memory-mapped array directly\n",
                "fp = np.memmap('temp_memmap.dat', dtype='float32', mode='w+', shape=(1000, 1000))\n",
                "\n",
                "# Write data\n",
                "fp[:] = np.random.rand(1000, 1000)\n",
                "\n",
                "# Flush to disk\n",
                "fp.flush()\n",
                "\n",
                "print(f\"Memmap shape: {fp.shape}\")\n",
                "del fp  # Close"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. allow_pickle Parameter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Object arrays require pickle\n",
                "obj_arr = np.array([{'a': 1}, {'b': 2}], dtype=object)\n",
                "\n",
                "np.save('object_array.npy', obj_arr, allow_pickle=True)\n",
                "print(\"Saved object array\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Loading object arrays requires allow_pickle=True\n",
                "try:\n",
                "    loaded = np.load('object_array.npy', allow_pickle=False)\n",
                "except ValueError as e:\n",
                "    print(f\"Error without pickle: {e}\")\n",
                "\n",
                "# With pickle enabled\n",
                "loaded = np.load('object_array.npy', allow_pickle=True)\n",
                "print(f\"Loaded: {loaded}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Security note: \n",
                "# pickle can execute arbitrary code\n",
                "# Only load .npy files from trusted sources if allow_pickle=True"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Practical Patterns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pattern: Save model weights\n",
                "weights = {\n",
                "    'layer1': np.random.rand(784, 256),\n",
                "    'layer2': np.random.rand(256, 128),\n",
                "    'layer3': np.random.rand(128, 10)\n",
                "}\n",
                "\n",
                "np.savez_compressed('model_weights.npz', **weights)\n",
                "print(\"Model weights saved\")\n",
                "\n",
                "# Load\n",
                "with np.load('model_weights.npz') as data:\n",
                "    loaded_weights = {k: data[k] for k in data.keys()}\n",
                "    print(f\"Loaded layers: {list(loaded_weights.keys())}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pattern: Checkpoint with metadata\n",
                "checkpoint = {\n",
                "    'epoch': np.array(10),\n",
                "    'loss': np.array([0.5, 0.3, 0.2, 0.1]),\n",
                "    'weights': np.random.rand(100, 50),\n",
                "    'optimizer_state': np.random.rand(100)\n",
                "}\n",
                "\n",
                "np.savez('checkpoint.npz', **checkpoint)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pattern: Incremental save to memory-mapped file\n",
                "n_samples = 10000\n",
                "n_features = 100\n",
                "\n",
                "# Create empty memory-mapped file\n",
                "fp = np.memmap('dataset.dat', dtype='float32', mode='w+', \n",
                "               shape=(n_samples, n_features))\n",
                "\n",
                "# Simulate writing data in batches\n",
                "batch_size = 1000\n",
                "for i in range(0, n_samples, batch_size):\n",
                "    fp[i:i+batch_size] = np.random.rand(batch_size, n_features)\n",
                "    fp.flush()  # Ensure data is written\n",
                "\n",
                "print(f\"Dataset shape: {fp.shape}\")\n",
                "del fp"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Key Points Summary\n",
                "\n",
                "**Save/Load Functions:**\n",
                "- `np.save()`: Single array to .npy\n",
                "- `np.savez()`: Multiple arrays to .npz (uncompressed)\n",
                "- `np.savez_compressed()`: Compressed .npz\n",
                "- `np.load()`: Load .npy or .npz\n",
                "\n",
                "**Memory Mapping:**\n",
                "- Use `mmap_mode` in `np.load()` for large files\n",
                "- `np.memmap()` for direct memory-mapped arrays\n",
                "- Modes: 'r' (read), 'r+' (read-write), 'c' (copy-on-write)\n",
                "\n",
                "**Best Practices:**\n",
                "- Use `.npz` for related arrays\n",
                "- Use `_compressed` for large sparse/structured data\n",
                "- Use memory mapping for arrays larger than RAM"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Interview Tips\n",
                "\n",
                "**Q1: Difference between .npy and .npz?**\n",
                "> - `.npy`: Single array, binary format\n",
                "> - `.npz`: Multiple arrays in a zip archive, accessed like dictionary\n",
                "\n",
                "**Q2: When to use savez_compressed?**\n",
                "> When file size matters and data is compressible (sparse arrays, repeated values). Trade-off is slower save/load due to compression.\n",
                "\n",
                "**Q3: What is memory mapping and when to use it?**\n",
                "> Memory mapping accesses file directly from disk without loading entirely into RAM. Use for arrays larger than available memory or when only accessing portions.\n",
                "\n",
                "**Q4: Why is allow_pickle a security concern?**\n",
                "> Pickle can execute arbitrary code during deserialization. Malicious .npy files could run harmful code when loaded."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Cleanup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clean up test files\n",
                "import glob\n",
                "\n",
                "for f in glob.glob('*.npy') + glob.glob('*.npz') + glob.glob('*.dat'):\n",
                "    os.remove(f)\n",
                "    print(f\"Removed: {f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Next Notebook\n",
                "**02_working_with_text_files.ipynb** - Reading and writing text/CSV files with NumPy."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}