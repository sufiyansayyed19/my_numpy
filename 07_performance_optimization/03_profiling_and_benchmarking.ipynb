{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Profiling and Benchmarking\n",
                "\n",
                "**Module 07 | Notebook 03**\n",
                "\n",
                "---\n",
                "\n",
                "## Objective\n",
                "By the end of this notebook, you will master:\n",
                "- Timing code execution accurately\n",
                "- Using %timeit magic\n",
                "- Profiling with cProfile and line_profiler\n",
                "- Memory profiling\n",
                "- Identifying bottlenecks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import time\n",
                "np.set_printoptions(precision=3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Basic Timing with time module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple timing\n",
                "arr = np.random.rand(10000000)\n",
                "\n",
                "start = time.time()\n",
                "result = arr.sum()\n",
                "end = time.time()\n",
                "\n",
                "print(f\"Elapsed: {end - start:.6f} seconds\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Better: time.perf_counter (higher resolution)\n",
                "start = time.perf_counter()\n",
                "result = arr.sum()\n",
                "elapsed = time.perf_counter() - start\n",
                "\n",
                "print(f\"Elapsed: {elapsed:.9f} seconds\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run multiple times for stable measurement\n",
                "def benchmark(func, *args, n_runs=10):\n",
                "    \"\"\"Run function multiple times and return stats.\"\"\"\n",
                "    times = []\n",
                "    for _ in range(n_runs):\n",
                "        start = time.perf_counter()\n",
                "        func(*args)\n",
                "        times.append(time.perf_counter() - start)\n",
                "    \n",
                "    times = np.array(times)\n",
                "    return {\n",
                "        'mean': times.mean(),\n",
                "        'std': times.std(),\n",
                "        'min': times.min(),\n",
                "        'max': times.max()\n",
                "    }\n",
                "\n",
                "stats = benchmark(np.sum, arr)\n",
                "print(f\"Mean: {stats['mean']*1000:.3f}ms +/- {stats['std']*1000:.3f}ms\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. IPython %timeit Magic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %timeit: runs code multiple times, reports best result\n",
                "arr = np.random.rand(1000000)\n",
                "\n",
                "%timeit arr.sum()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare operations\n",
                "%timeit arr.sum()     # Built-in\n",
                "%timeit np.sum(arr)   # Function call"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Control number of runs\n",
                "%timeit -n 100 -r 5 arr ** 2  # 100 loops, 5 runs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%timeit for multi-line code\n",
                "%%timeit\n",
                "a = np.random.rand(1000)\n",
                "b = np.random.rand(1000)\n",
                "c = a + b"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get result as object\n",
                "result = %timeit -o arr.sum()\n",
                "print(f\"Best: {result.best*1000:.4f}ms\")\n",
                "print(f\"Worst: {result.worst*1000:.4f}ms\")\n",
                "print(f\"All times: {[f'{t*1000:.4f}ms' for t in result.timings[:3]]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. cProfile for Function Profiling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cProfile\n",
                "import pstats\n",
                "from io import StringIO\n",
                "\n",
                "def complex_operation():\n",
                "    \"\"\"Function with multiple operations.\"\"\"\n",
                "    arr = np.random.rand(1000, 1000)\n",
                "    \n",
                "    # Various operations\n",
                "    result = np.dot(arr, arr.T)\n",
                "    result = np.linalg.eigvals(result[:10, :10])\n",
                "    result = np.fft.fft(arr[0])\n",
                "    \n",
                "    return result\n",
                "\n",
                "# Profile\n",
                "profiler = cProfile.Profile()\n",
                "profiler.enable()\n",
                "\n",
                "complex_operation()\n",
                "\n",
                "profiler.disable()\n",
                "\n",
                "# Print stats\n",
                "stats = pstats.Stats(profiler)\n",
                "stats.sort_stats('cumulative')\n",
                "stats.print_stats(10)  # Top 10 functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# IPython magic: %prun\n",
                "%prun complex_operation()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Line Profiler (if installed)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Line profiler shows time per line\n",
                "# Install: pip install line_profiler\n",
                "\n",
                "def analyze_data(n):\n",
                "    # Create data\n",
                "    data = np.random.rand(n, n)\n",
                "    \n",
                "    # Normalize rows\n",
                "    row_means = data.mean(axis=1, keepdims=True)\n",
                "    row_stds = data.std(axis=1, keepdims=True)\n",
                "    normalized = (data - row_means) / row_stds\n",
                "    \n",
                "    # Compute correlation\n",
                "    corr = np.corrcoef(normalized)\n",
                "    \n",
                "    # Get eigenvalues\n",
                "    eigenvalues = np.linalg.eigvalsh(corr)\n",
                "    \n",
                "    return eigenvalues\n",
                "\n",
                "# Manual line timing\n",
                "import time\n",
                "\n",
                "n = 1000\n",
                "\n",
                "start = time.perf_counter(); data = np.random.rand(n, n); t1 = time.perf_counter() - start\n",
                "start = time.perf_counter(); row_means = data.mean(axis=1, keepdims=True); t2 = time.perf_counter() - start\n",
                "start = time.perf_counter(); row_stds = data.std(axis=1, keepdims=True); t3 = time.perf_counter() - start\n",
                "start = time.perf_counter(); normalized = (data - row_means) / row_stds; t4 = time.perf_counter() - start\n",
                "start = time.perf_counter(); corr = np.corrcoef(normalized); t5 = time.perf_counter() - start\n",
                "start = time.perf_counter(); eigenvalues = np.linalg.eigvalsh(corr); t6 = time.perf_counter() - start\n",
                "\n",
                "print(f\"Create data: {t1*1000:.2f}ms\")\n",
                "print(f\"Row means: {t2*1000:.2f}ms\")\n",
                "print(f\"Row stds: {t3*1000:.2f}ms\")\n",
                "print(f\"Normalize: {t4*1000:.2f}ms\")\n",
                "print(f\"Correlation: {t5*1000:.2f}ms\")\n",
                "print(f\"Eigenvalues: {t6*1000:.2f}ms\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Memory Profiling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check array memory usage\n",
                "arr = np.random.rand(1000, 1000)\n",
                "\n",
                "print(f\"Array shape: {arr.shape}\")\n",
                "print(f\"Dtype: {arr.dtype}\")\n",
                "print(f\"Size: {arr.size} elements\")\n",
                "print(f\"Memory: {arr.nbytes / 1e6:.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Estimate memory for operations\n",
                "def estimate_memory(shape, dtype, n_copies=1):\n",
                "    \"\"\"Estimate memory needed.\"\"\"\n",
                "    itemsize = np.dtype(dtype).itemsize\n",
                "    total_bytes = np.prod(shape) * itemsize * n_copies\n",
                "    return total_bytes / 1e6  # MB\n",
                "\n",
                "# Matrix multiply: A @ B creates result same size as output\n",
                "n = 5000\n",
                "print(f\"Two {n}x{n} float64 matrices: {estimate_memory((n,n), 'float64', 2):.0f} MB\")\n",
                "print(f\"Their product: {estimate_memory((n,n), 'float64', 1):.0f} MB\")\n",
                "print(f\"Total: {estimate_memory((n,n), 'float64', 3):.0f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Track memory with tracemalloc\n",
                "import tracemalloc\n",
                "\n",
                "tracemalloc.start()\n",
                "\n",
                "# Memory-intensive operation\n",
                "arr = np.random.rand(2000, 2000)\n",
                "result = arr @ arr.T\n",
                "\n",
                "current, peak = tracemalloc.get_traced_memory()\n",
                "tracemalloc.stop()\n",
                "\n",
                "print(f\"Current memory: {current / 1e6:.1f} MB\")\n",
                "print(f\"Peak memory: {peak / 1e6:.1f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Memory-profiler (if installed)\n",
                "# pip install memory_profiler\n",
                "# Use @profile decorator and run with: python -m memory_profiler script.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Comparing Implementations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Framework for comparing implementations\n",
                "def compare_implementations(*funcs, args=(), n_runs=10):\n",
                "    \"\"\"Compare multiple implementations.\"\"\"\n",
                "    results = {}\n",
                "    \n",
                "    for func in funcs:\n",
                "        times = []\n",
                "        for _ in range(n_runs):\n",
                "            start = time.perf_counter()\n",
                "            func(*args)\n",
                "            times.append(time.perf_counter() - start)\n",
                "        \n",
                "        results[func.__name__] = {\n",
                "            'mean': np.mean(times),\n",
                "            'std': np.std(times)\n",
                "        }\n",
                "    \n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Compare normalization approaches\n",
                "arr = np.random.rand(1000, 1000)\n",
                "\n",
                "def normalize_loop(arr):\n",
                "    result = np.empty_like(arr)\n",
                "    for i in range(arr.shape[0]):\n",
                "        row = arr[i]\n",
                "        result[i] = (row - row.mean()) / row.std()\n",
                "    return result\n",
                "\n",
                "def normalize_vectorized(arr):\n",
                "    mean = arr.mean(axis=1, keepdims=True)\n",
                "    std = arr.std(axis=1, keepdims=True)\n",
                "    return (arr - mean) / std\n",
                "\n",
                "def normalize_einsum(arr):\n",
                "    # Alternative using einsum\n",
                "    n = arr.shape[1]\n",
                "    mean = np.einsum('ij->i', arr)[:, None] / n\n",
                "    return (arr - mean) / np.sqrt(np.einsum('ij,ij->i', arr - mean, arr - mean)[:, None] / n)\n",
                "\n",
                "results = compare_implementations(\n",
                "    normalize_loop,\n",
                "    normalize_vectorized,\n",
                "    normalize_einsum,\n",
                "    args=(arr,)\n",
                ")\n",
                "\n",
                "for name, stats in results.items():\n",
                "    print(f\"{name}: {stats['mean']*1000:.2f}ms +/- {stats['std']*1000:.2f}ms\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Identifying Bottlenecks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pattern: Incremental profiling\n",
                "import time\n",
                "\n",
                "class Timer:\n",
                "    \"\"\"Context manager for timing code blocks.\"\"\"\n",
                "    def __init__(self, name=\"\"):\n",
                "        self.name = name\n",
                "        \n",
                "    def __enter__(self):\n",
                "        self.start = time.perf_counter()\n",
                "        return self\n",
                "    \n",
                "    def __exit__(self, *args):\n",
                "        self.elapsed = time.perf_counter() - self.start\n",
                "        print(f\"{self.name}: {self.elapsed*1000:.2f}ms\")\n",
                "\n",
                "# Usage\n",
                "with Timer(\"Data creation\"):\n",
                "    data = np.random.rand(2000, 2000)\n",
                "\n",
                "with Timer(\"Matrix multiply\"):\n",
                "    result = data @ data.T\n",
                "\n",
                "with Timer(\"SVD\"):\n",
                "    u, s, v = np.linalg.svd(result[:100, :100])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cumulative timer\n",
                "class CumulativeTimer:\n",
                "    \"\"\"Track time across multiple calls.\"\"\"\n",
                "    times = {}\n",
                "    \n",
                "    @classmethod\n",
                "    def start(cls, name):\n",
                "        if name not in cls.times:\n",
                "            cls.times[name] = {'total': 0, 'count': 0}\n",
                "        cls._current_start = time.perf_counter()\n",
                "        cls._current_name = name\n",
                "    \n",
                "    @classmethod\n",
                "    def stop(cls):\n",
                "        elapsed = time.perf_counter() - cls._current_start\n",
                "        cls.times[cls._current_name]['total'] += elapsed\n",
                "        cls.times[cls._current_name]['count'] += 1\n",
                "    \n",
                "    @classmethod\n",
                "    def report(cls):\n",
                "        for name, data in cls.times.items():\n",
                "            avg = data['total'] / data['count'] * 1000\n",
                "            print(f\"{name}: {data['total']*1000:.2f}ms total, {avg:.2f}ms avg ({data['count']} calls)\")\n",
                "\n",
                "# Simulate workload\n",
                "CumulativeTimer.times = {}  # Reset\n",
                "\n",
                "for i in range(10):\n",
                "    CumulativeTimer.start('random')\n",
                "    data = np.random.rand(500, 500)\n",
                "    CumulativeTimer.stop()\n",
                "    \n",
                "    CumulativeTimer.start('compute')\n",
                "    result = np.linalg.svd(data)\n",
                "    CumulativeTimer.stop()\n",
                "\n",
                "CumulativeTimer.report()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Best Practices Checklist"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Performance checklist\n",
                "checklist = \"\"\"\n",
                "BEFORE OPTIMIZING:\n",
                "[ ] Is the code correct?\n",
                "[ ] Is optimization necessary? (80/20 rule)\n",
                "[ ] Have you profiled to find the bottleneck?\n",
                "\n",
                "COMMON OPTIMIZATIONS:\n",
                "[ ] Using vectorized operations (no Python loops)\n",
                "[ ] Avoiding temporary arrays (in-place, out=)\n",
                "[ ] Using specialized functions (np.dot, np.linalg)\n",
                "[ ] Choosing appropriate dtype\n",
                "[ ] Ensuring contiguous memory layout\n",
                "[ ] Processing in cache-friendly order\n",
                "\n",
                "AFTER OPTIMIZING:\n",
                "[ ] Is the code still correct? (test!)\n",
                "[ ] Is the speedup significant?\n",
                "[ ] Is the code still readable?\n",
                "\"\"\"\n",
                "print(checklist)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Key Points Summary\n",
                "\n",
                "**Timing:**\n",
                "- Use `time.perf_counter()` for high resolution\n",
                "- Run multiple times for stable measurements\n",
                "- Use `%timeit` in Jupyter for convenience\n",
                "\n",
                "**Profiling:**\n",
                "- `cProfile` / `%prun`: function-level timing\n",
                "- `line_profiler`: line-by-line timing\n",
                "- `tracemalloc`: memory tracking\n",
                "\n",
                "**Workflow:**\n",
                "1. Write correct code first\n",
                "2. Profile to find bottleneck\n",
                "3. Optimize the bottleneck\n",
                "4. Verify correctness\n",
                "5. Measure improvement"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Interview Tips\n",
                "\n",
                "**Q1: How do you profile NumPy code?**\n",
                "> Use `%timeit` for quick timing, `cProfile` for function-level analysis, `line_profiler` for line-by-line, and `tracemalloc` for memory. Always run multiple times for stable results.\n",
                "\n",
                "**Q2: What's the optimization workflow?**\n",
                "> 1. Write working code first\n",
                "> 2. Profile to find actual bottleneck (not assumed)\n",
                "> 3. Optimize the hot path\n",
                "> 4. Verify correctness with tests\n",
                "> 5. Measure actual speedup\n",
                "\n",
                "**Q3: Why profile before optimizing?**\n",
                "> Most time is spent in small portion of code. Optimizing the wrong part wastes effort. Profiling identifies actual bottlenecks.\n",
                "\n",
                "**Q4: How do you measure memory usage?**\n",
                "> Check `arr.nbytes` for array size, use `tracemalloc` for runtime tracking, or `memory_profiler` for line-by-line memory usage."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Practice Exercises"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 1: Profile and optimize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Profile this function and identify the bottleneck\n",
                "def slow_function(n):\n",
                "    result = np.zeros((n, n))\n",
                "    for i in range(n):\n",
                "        for j in range(n):\n",
                "            result[i, j] = np.sqrt(i**2 + j**2)\n",
                "    return result\n",
                "\n",
                "# Time it\n",
                "start = time.perf_counter()\n",
                "slow_function(200)\n",
                "print(f\"Slow version: {(time.perf_counter()-start)*1000:.1f}ms\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Solution: Vectorize\n",
                "def fast_function(n):\n",
                "    i = np.arange(n)[:, np.newaxis]\n",
                "    j = np.arange(n)[np.newaxis, :]\n",
                "    return np.sqrt(i**2 + j**2)\n",
                "\n",
                "start = time.perf_counter()\n",
                "fast_function(200)\n",
                "print(f\"Fast version: {(time.perf_counter()-start)*1000:.3f}ms\")\n",
                "\n",
                "# Verify\n",
                "print(f\"Results match: {np.allclose(slow_function(50), fast_function(50))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 2: Compare memory usage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare memory of float64 vs float32 for a large array\n",
                "n = 10000\n",
                "\n",
                "arr64 = np.random.rand(n, n)\n",
                "arr32 = arr64.astype(np.float32)\n",
                "\n",
                "print(f\"float64: {arr64.nbytes / 1e9:.2f} GB\")\n",
                "print(f\"float32: {arr32.nbytes / 1e9:.2f} GB\")\n",
                "print(f\"Savings: {(arr64.nbytes - arr32.nbytes) / 1e9:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Module 07 Complete!\n",
                "\n",
                "You have mastered Performance Optimization:\n",
                "- Memory Layout and Views\n",
                "- Vectorization Best Practices\n",
                "- Profiling and Benchmarking\n",
                "\n",
                "**Next Module:** 08_practice_problems - Put it all together with exercises and challenges!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}